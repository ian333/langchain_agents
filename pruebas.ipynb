{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HOla\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "from datetime import datetime, timedelta\n",
    "import uuid\n",
    "import streamlit as st\n",
    "\n",
    "\n",
    "import os\n",
    "from supabase import create_client\n",
    "\n",
    "from bson import ObjectId\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conexión a MongoDB (por defecto se conecta al localhost y puerto 27017)\n",
    "client = MongoClient(f\"mongodb+srv://SkillsGPT:{st.secrets.MONGO_PASSWORD}@cluster0.keobhw0.mongodb.net/?retryWrites=true&w=majority\")\n",
    "#db = client['user_database']  # Aquí creamos/seleccionamos la base de datos\n",
    "\n",
    "db = client['SkillsGPT-v2']\n",
    "\n",
    "# Colecciones\n",
    "profesores = db['profesores']\n",
    "cursos = db['cursos']\n",
    "estudiantes = db['estudiantes']\n",
    "users_collection = db['users']\n",
    "tokens_collection = db['password_reset_tokens']\n",
    "collection = db['chats']\n",
    "auth_tokens_collection = db['auth_tokens_collection']\n",
    "email_verification_tokens = db['email_verification_tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_collection.find_one({'username':'ian333'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printAS(result):\n",
    "\n",
    "  \"\"\"\n",
    "  Prints the 'answer' (A) and the sources (S) from the 'source_documents' list in the result dictionary.\n",
    "  Adapted for RetrievalQAWithSourcesChain\n",
    "\n",
    "  Parameters:\n",
    "  result (dict): A dictionary containing keys 'answer' and 'source_documents'. The 'source_documents'\n",
    "                 key should be a list of Document objects, each having a 'metadata' dictionary with\n",
    "                 a 'source' key.\n",
    "\n",
    "  Output:\n",
    "  Prints the 'answer' followed by the phrase \"To know more, check:\" and then each source URL\n",
    "  on a new line. If no source URL is found, it prints 'No source available.' for that entry.\n",
    "  \"\"\"\n",
    "  answer=\"\\nTo know more, check:\"\n",
    "  # Print the 'answer'\n",
    "  print(result.get('answer', 'No answer available.'))\n",
    "  answer=answer\n",
    "  # Print the introductory phrase for sources\n",
    "  print(\"\\nTo know more, check:\")\n",
    "\n",
    "  # Check if 'source_documents' key exists in the result\n",
    "  if 'source_documents' in result:\n",
    "    for document in result['source_documents']:\n",
    "      # Get the source title and URL from the metadata of each document\n",
    "      source_title = document.metadata.get('title', 'No title available.')\n",
    "      source_url = document.metadata.get('source', 'No url available.')\n",
    "      print(source_title,' - ',source_url)\n",
    "      answer=answer+source_title+' - '+source_url\n",
    "  else:\n",
    "    print(\"No source documents available.\")\n",
    "  return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set API keys\n",
    "\n",
    "# Assembly AI\n",
    "# import assemblyai as aai\n",
    "# aai.settings.api_key = \"26f195ae63cf434280dd530fb61d6981\"\n",
    "\n",
    "# OpenAI\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"***REMOVED***PrbQpTkArdcDp4BItWXdT3BlbkFJN76nqGc4qtQzGYXxVDfO\"\n",
    "\n",
    "# Pinecone\n",
    "import pinecone\n",
    "pinecone.init(\n",
    "    api_key=\"71d56197-0de5-45f2-ac9a-c58c3cfb761f\", # find at app.pinecone.io | rag\n",
    "    environment=\"us-west1-gcp-free\" # next to api key in console\n",
    ")\n",
    "\n",
    "# create index\n",
    "index_name = 'skillsgpt'\n",
    "if index_name not in pinecone.list_indexes():\n",
    "    pinecone.create_index(\n",
    "        index_name,\n",
    "        dimension=1536,\n",
    "        metric='cosine',\n",
    "    )\n",
    "\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from decouple import config\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "openai_api_key=config('OPEN_AI_KEY')\n",
    "\n",
    "# connect to the index & vectorstore\n",
    "pinecone_index = pinecone.Index(index_name)\n",
    "vectorstore = Pinecone.from_existing_index(index_name, OpenAIEmbeddings())\n",
    "llm = ChatOpenAI(verbose=True,model=\"gpt-3.5-turbo\",openai_api_key=openai_api_key,temperature=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import Tool\n",
    "\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "qa = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0),\n",
    "retriever = vectorstore.as_retriever(search_type=\"mmr\", search_kwargs={'fetch_k': 3}),\n",
    "return_source_documents=True,\n",
    "verbose=True,\n",
    ")\n",
    "def transcript_videos(query):\n",
    "    query = \"How can you become a partner at New York Life?\"\n",
    "    result = qa(query)\n",
    "    # printAS(result)\n",
    "    return result[\"answer\"] +printAS(result)\n",
    "\n",
    "    # qa = RetrievalQA.from_llm(llm=llm, retriever=vectorstore)\n",
    "    # qa.return_source_documents = True\n",
    "\n",
    "\n",
    "    # try:\n",
    "    #     result =qa(query)\n",
    "    # except:\n",
    "    #     result={'source_documents':'No tengo informacion para responder tu pregunta , o podrias reformular tu pregunta ?'}\n",
    "    \n",
    "    # print(result)\n",
    "\n",
    "    # return result['source_documents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript_videos(\"New york life\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "tools = [\n",
    "\n",
    "             Tool.from_function(\n",
    "                 name='Transcript Videos',\n",
    "                 func=transcript_videos,\n",
    "                 description=(\"\"\" Usa esta herramienta , con esto tienes acceso al texto de videos transcritos acerca de NEw York Life , estas entrenando a sus agentes\"\"\" \n",
    "                 )\n",
    "                 )\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentType\n",
    "from langchain.agents.initialize import initialize_agent\n",
    "from langchain.chains.conversation.memory import ConversationBufferMemory\n",
    "\n",
    "agent = initialize_agent(\n",
    "            agent=AgentType.CHAT_CONVERSATIONAL_REACT_DESCRIPTION,\n",
    "            tools=tools,\n",
    "            llm=llm,\n",
    "            verbose=True,\n",
    "            max_iterations=3,\n",
    "            early_stopping_method='generate',\n",
    "            memory=ConversationBufferMemory(memory_key='chat_history', return_messages=True),\n",
    "            handle_parsing_errors=True\n",
    "            \n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.agent.llm_chain.prompt.messages[0].prompt.template=\"\"\"Assistant es un modelo de lenguaje avanzado entrenado por OpenAI, diseñado para funcionar como un tutor inteligente. Su principal objetivo es proporcionar asistencia detallada y precisa, basándose en una amplia gama de fuentes confiables. Como tutor, Assistant se enfoca en profundizar en los temas, ofreciendo explicaciones claras y bien fundamentadas.\n",
    "\n",
    "El Assistant está programado para priorizar la seguridad y la precisión en sus respuestas. En situaciones donde la información incorrecta podría tener consecuencias graves, el modelo se esfuerza por verificar los hechos y citar fuentes relevantes y confiables. Se alienta a los usuarios a proporcionar enlaces o referencias a material de calidad para que Assistant pueda utilizarlos en sus respuestas.\n",
    "\n",
    "El modelo utiliza su capacidad para procesar y comprender grandes cantidades de texto, permitiéndole acceder a un amplio espectro de conocimientos. Sin embargo, siempre se recomienda a los usuarios verificar la información proporcionada y considerarla como parte de una investigación más amplia.\n",
    "\n",
    "tienes auxiliares que te daran informacion clara , esta informacion de tus herramientas utiliza la mayo informacion posible que te den si te dan links siempre da los links\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.run(\"Hola oye como me puedo convertir en un agente,que dicen los videos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..')\n",
    "from videos_transcript import agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.run(\"Hola oye como me puedo convertir en un agente,que dicen los videos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segunda Parte Codigo Luigi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Luigi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U yt-dlp # tools for Youtube\n",
    "%pip install assemblyai # transcript\n",
    "%pip install langchain==0.0.354 # 8/1/24 Langchain 0.1.0 is still not stable\n",
    "%pip install openai\n",
    "%pip install sentence-transformers # chunks\n",
    "%pip install pinecone-client\n",
    "%pip install tiktoken # parsing chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "# yt_dlp\n",
    "import yt_dlp\n",
    "\n",
    "# pinecone\n",
    "import pinecone\n",
    "\n",
    "# langchain\n",
    "#from langchain.chains import RetrievalQA\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import AssemblyAIAudioTranscriptLoader\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "#from langchain_openai import OpenAIEmbeddings # for when langchain 0.1.0 gets stable\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Pinecone\n",
    "# Set API keys\n",
    "\n",
    "# Assembly AI\n",
    "import assemblyai as aai\n",
    "aai.settings.api_key = \"26f195ae63cf434280dd530fb61d6981\"\n",
    "\n",
    "# OpenAI\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"***REMOVED***PrbQpTkArdcDp4BItWXdT3BlbkFJN76nqGc4qtQzGYXxVDfO\"\n",
    "\n",
    "# Pinecone\n",
    "pinecone.init(\n",
    "    api_key=\"71d56197-0de5-45f2-ac9a-c58c3cfb761f\", # find at app.pinecone.io | rag\n",
    "    environment=\"us-west1-gcp-free\" # next to api key in console\n",
    ")\n",
    "\n",
    "# create index\n",
    "index_name = 'skillsgpt'\n",
    "if index_name not in pinecone.list_indexes():\n",
    "    pinecone.create_index(\n",
    "        index_name,\n",
    "        dimension=1536,\n",
    "        metric='cosine',\n",
    "    )\n",
    "\n",
    "# connect to the index & vectorstore\n",
    "pinecone_index = pinecone.Index(index_name)\n",
    "vectorstore = Pinecone.from_existing_index(index_name,namespace=\"Name\", OpenAIEmbeddings())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pinecone.list_indexes())\n",
    "print(vectorstore.namespaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "\n",
    "def getTranscript_yt(YT_URL):\n",
    "  \"\"\"\n",
    "  Extracts the best quality audio-only url from a given YouTube video.\n",
    "\n",
    "  This function uses the yt_dlp library to extract information from a YouTube video URL.\n",
    "  It iterates through the available formats in descending order of quality and returns the URL\n",
    "  for the highest quality audio-only stream in M4A format.\n",
    "\n",
    "  Parameters:\n",
    "  URL (str): The URL of the YouTube video from which the audio URL is to be extracted.\n",
    "\n",
    "  Returns:\n",
    "  tuple: A tuple containing the YouTube video URL (str), the title of the video (str), and the URL\n",
    "         of the highest quality audio-only M4A stream (str). If no suitable audio stream is found,\n",
    "         the corresponding value in the tuple will be None.\n",
    "\n",
    "  Note:\n",
    "  - The function does not download the video or audio; it only extracts the URL.\n",
    "  - It requires the yt_dlp library to be installed and available for import.\n",
    "  \"\"\"\n",
    "  with yt_dlp.YoutubeDL() as ydl:\n",
    "    info = ydl.extract_info(YT_URL, download=False)\n",
    "    YT_title = info.get('title', None)\n",
    "\n",
    "  # formats are already sorted worst to best\n",
    "  # --> iterate in reverse and take the first audio to get the best\n",
    "  for format in info[\"formats\"][::-1]:\n",
    "    if format[\"resolution\"] == \"audio only\" and format[\"ext\"] == \"m4a\":\n",
    "      audio_url = format[\"url\"]\n",
    "      break\n",
    "\n",
    "  return YT_URL, YT_title, audio_url\n",
    "\n",
    "\n",
    "def url2docs(YT_URL,YT_title,audio_url):\n",
    "  \"\"\"\n",
    "  Converts an audio or video URL to a transcribed document using AssemblyAI.\n",
    "\n",
    "  This function uses AssemblyAI to transcribe audio or video content from a provided URL.\n",
    "  It automatically detects the language of the content and highlights important parts of the transcription.\n",
    "\n",
    "  Parameters:\n",
    "  url (str): The URL of the audio or video file to be transcribed.\n",
    "\n",
    "  Returns:\n",
    "  Document: An object containing the transcribed text of the audio or video file.\n",
    "            The exact format and contents of this object depend on the AssemblyAIAudioTranscriptLoader's implementation.\n",
    "\n",
    "  Note:\n",
    "  - The function requires the 'AssemblyAIAudioTranscriptLoader' class to be defined elsewhere in the code.\n",
    "  - The transcription config is set to detect the language automatically and to highlight key parts of the transcription.\n",
    "  - This function assumes that necessary credentials and configurations for accessing AssemblyAI services are already set up.\n",
    "  \"\"\"\n",
    "  # https://www.assemblyai.com/blog/automatically-determine-video-sections-with-ai-using-python/\n",
    "  config = aai.TranscriptionConfig(\n",
    "      language_detection=True,\n",
    "      auto_highlights=True,\n",
    "      )\n",
    "\n",
    "  loader = AssemblyAIAudioTranscriptLoader(audio_url,config=config)\n",
    "  docs = loader.load()\n",
    "\n",
    "  # Cut metadata\n",
    "  #docs[0].metadata = {\"source\": URL, \"words\":docs[0].metadata[\"words\"]}\n",
    "  #docs[0].metadata = {\"source\": YT_URL, \"title\": YT_title, \"words\":docs[0].metadata[\"words\"]}\n",
    "  docs[0].metadata = {\"source\": YT_URL, \"title\": YT_title}\n",
    "\n",
    "  return docs\n",
    "\n",
    "\n",
    "def docs2pineconeDB(docs,index_name,namespace:str):\n",
    "  \"\"\"\n",
    "  Processes a collection of documents and stores them in a Pinecone database with embeddings.\n",
    "\n",
    "  This function splits each document into smaller chunks using a RecursiveCharacterTextSplitter,\n",
    "  then generates embeddings for these chunks using the OpenAI 'text-embedding-ada-002' model.\n",
    "  Finally, it stores these embedded chunks in a Pinecone database.\n",
    "\n",
    "  Parameters:\n",
    "  docs (list of str): A list of documents to be processed and stored in the Pinecone database.\n",
    "  index_name (str): The name of the Pinecone index where the documents will be stored.\n",
    "\n",
    "  Returns:\n",
    "  PineconeIndex: An object representing the Pinecone database index containing the processed documents.\n",
    "                  This index is suitable for search and retrieval tasks.\n",
    "\n",
    "  Note:\n",
    "  - The function assumes that the Pinecone and OpenAI modules, and the necessary models and classes\n",
    "    (RecursiveCharacterTextSplitter, OpenAIEmbeddings) are already imported and available.\n",
    "  - Ensure that the Pinecone API key and other configuration settings are correctly set up before calling this function.\n",
    "  - The chunk size for document splitting and embedding model can be adjusted as needed.\n",
    "  \"\"\"\n",
    "  text_splitter = RecursiveCharacterTextSplitter(chunk_size=700, chunk_overlap=0)\n",
    "  texts = text_splitter.split_documents(docs)\n",
    "\n",
    "  database = Pinecone.from_documents(\n",
    "      docs,\n",
    "      OpenAIEmbeddings(),\n",
    "      index_name=index_name,\n",
    "      namespace=namespace\n",
    "      )\n",
    "\n",
    "  print('Transcript loaded to pinecone')\n",
    "  #return database\n",
    "\n",
    "\n",
    "def printAS(result):\n",
    "  \"\"\"\n",
    "  Prints the 'answer' (A) and the sources (S) from the 'source_documents' list in the result dictionary.\n",
    "  Adapted for RetrievalQAWithSourcesChain\n",
    "\n",
    "  Parameters:\n",
    "  result (dict): A dictionary containing keys 'answer' and 'source_documents'. The 'source_documents'\n",
    "                 key should be a list of Document objects, each having a 'metadata' dictionary with\n",
    "                 a 'source' key.\n",
    "\n",
    "  Output:\n",
    "  Prints the 'answer' followed by the phrase \"To know more, check:\" and then each source URL\n",
    "  on a new line. If no source URL is found, it prints 'No source available.' for that entry.\n",
    "  \"\"\"\n",
    "  # Print the 'answer'\n",
    "  print(result.get('answer', 'No answer available.'))\n",
    "\n",
    "  # Print the introductory phrase for sources\n",
    "  print(\"\\nTo know more, check:\")\n",
    "\n",
    "  # Check if 'source_documents' key exists in the result\n",
    "  if 'source_documents' in result:\n",
    "    for document in result['source_documents']:\n",
    "      # Get the source title and URL from the metadata of each document\n",
    "      source_title = document.metadata.get('title', 'No title available.')\n",
    "      source_url = document.metadata.get('source', 'No url available.')\n",
    "      print(source_title,' - ',source_url)\n",
    "  else:\n",
    "    print(\"No source documents available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conexion a Supabase para extraer urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from supabase import create_client\n",
    "from decouple import config\n",
    "\n",
    "url_supabase = config('SUPABASE_URL')\n",
    "key_supabase = config('SUPABASE_KEY')\n",
    "supabase = create_client(url_supabase, key_supabase)\n",
    "\n",
    "youtube_urls = supabase.table('responses_tb').select(\"*\").eq('id','35adb689-1f8d-495c-9b76-c39ebce1d9a5').execute().data[0]\n",
    "youtube_urls['reference_videos']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment if you want to add transcripts to pinecone\n",
    "# URLs considered were from the NYL Insurance playlist:\n",
    "# https://www.youtube.com/watch?v=-Il6GajUWu0&list=PLuJnN3LjdmLMXTZi55jqkw_1TlAKA3ehX\n",
    "\n",
    "# List of YouTube URLs. Adjust it when preparing dataset\n",
    "# youtube_urls = [\n",
    "#     'https://www.youtube.com/watch?v=-Il6GajUWu0',\n",
    "#     'https://www.youtube.com/watch?v=fEwr_kmvfyY',\n",
    "#     'https://www.youtube.com/watch?v=cMDADYQiIGA',\n",
    "#     'https://www.youtube.com/watch?v=RAtgAZ6q2w4',\n",
    "#     'https://www.youtube.com/watch?v=2lcSp4_BUgc',\n",
    "#     'https://www.youtube.com/watch?v=buiYKkM0xqs',\n",
    "#     'https://www.youtube.com/watch?v=PXfOUOke6Lw',\n",
    "#     'https://www.youtube.com/watch?v=SEps-sO9GyM',\n",
    "# ]\n",
    "youtube_urls=youtube_urls['reference_videos']\n",
    "#youtube_urls = ['https://www.youtube.com/watch?v=UCzKeHPYceU']\n",
    "\n",
    "# Loop through each URL\n",
    "for yt_url in youtube_urls:\n",
    "    print(yt_url)\n",
    "    # Get transcript details from YouTube URL\n",
    "    URL, title, url = getTranscript_yt(yt_url)\n",
    "\n",
    "    # Convert URL to documents\n",
    "    docs = url2docs(URL, title, url)\n",
    "\n",
    "    # Store documents in Pinecone database\n",
    "    docs2pineconeDB(docs, 'skillsgpt',namespace=\"Transcript/35adb689-1f8d-495c-9b76-c39ebce1d9a5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history=[\n",
    "  {\n",
    "    \"Prompt\": \"Quien faraday?\",\n",
    "    \"Answer\": \"Michael Faraday fue un físico y químico británico que vivió en el siglo XIX. Es conocido por sus importantes contribuciones en el campo de la electromagnetismo y la electroquímica. Faraday descubrió la inducción electromagnética, estableciendo la base para el desarrollo de los generadores y motores eléctricos. También formuló las leyes de la electrólisis y realizó experimentos pioneros en electroquímica. Sus investigaciones sentaron las bases para el desarrollo de la teoría electromagnética de James Clerk Maxwell y tuvieron un impacto significativo en el avance de la tecnología eléctrica. Faraday es considerado uno de los científicos más influyentes de la historia.\"\n",
    "  },\n",
    "  {\n",
    "    \"Prompt\": \"Quien es el famoso jose hoyo?\",\n",
    "    \"Answer\": \"No tengo información sobre una persona famosa llamada José Hoyo. Puede ser que se trate de alguien que no sea ampliamente conocido o que no sea reconocido a nivel internacional.\"\n",
    "  },\n",
    "  {\n",
    "    \"Prompt\": \"quien es newton?\",\n",
    "    \"Answer\": \"Isaac Newton fue un físico, matemático y astrónomo inglés que vivió en el siglo XVII. Es considerado uno de los científicos más influyentes de la historia y es conocido por sus numerosas contribuciones a la física y las matemáticas. Newton formuló las leyes del movimiento, conocidas como las leyes de Newton, que son fundamentales para la mecánica clásica. También desarrolló la teoría de la gravitación universal, que explica la atracción entre los objetos debido a su masa. Además, Newton hizo importantes avances en el campo del cálculo y estableció las bases de la física moderna. Su trabajo sentó las bases para el desarrollo de la física clásica y su legado perdura hasta el día de hoy.\"\n",
    "  },\n",
    "  {\n",
    "    \"Prompt\": \"quien es newton?\",\n",
    "    \"Answer\": \"Isaac Newton fue un físico, matemático y astrónomo inglés que vivió en el siglo XVII. Es considerado uno de los científicos más influyentes de la historia y es conocido por sus numerosas contribuciones a la física y las matemáticas. Newton formuló las leyes del movimiento, conocidas como las leyes de Newton, que son fundamentales para la mecánica clásica. También desarrolló la teoría de la gravitación universal, que explica la atracción entre los objetos debido a su masa. Además, Newton hizo importantes avances en el campo del cálculo y estableció las bases de la física moderna. Su trabajo sentó las bases para el desarrollo de la física clásica y su legado perdura hasta el día de hoy.\"\n",
    "  },\n",
    "  {\n",
    "    \"Prompt\": \"Que significa f=ma?\\n\",\n",
    "    \"Answer\": \"La ecuación \\\"f = ma\\\" es conocida como la segunda ley de Newton y establece la relación entre la fuerza aplicada a un objeto, su masa y su aceleración. \\n\\nEn esta ecuación:\\n- \\\"f\\\" representa la fuerza neta aplicada sobre un objeto.\\n- \\\"m\\\" representa la masa del objeto.\\n- \\\"a\\\" representa la aceleración del objeto.\\n\\nLa ecuación establece que la fuerza neta aplicada sobre un objeto es igual al producto de su masa por su aceleración. Esto significa que si se aplica una fuerza neta sobre un objeto, este experimentará una aceleración proporcional a la magnitud de la fuerza y a la inversa de su masa. En otras palabras, cuanto mayor sea la fuerza aplicada sobre un objeto, mayor será su aceleración, y cuanto mayor sea la masa del objeto, menor será su aceleración para una misma fuerza aplicada.\"\n",
    "  },\n",
    "  {\n",
    "    \"Prompt\": \"quien es newton?\",\n",
    "    \"Answer\": \"Isaac Newton fue un físico, matemático y astrónomo inglés que vivió en el siglo XVII. Es considerado uno de los científicos más influyentes de la historia y es conocido por sus numerosas contribuciones a la física y las matemáticas. Newton formuló las leyes del movimiento, conocidas como las leyes de Newton, que son fundamentales para la mecánica clásica. También desarrolló la teoría de la gravitación universal, que explica la atracción entre los objetos debido a su masa. Además, Newton hizo importantes avances en el campo del cálculo y estableció las bases de la física moderna. Su trabajo sentó las bases para el desarrollo de la física clásica y su legado perdura hasta el día de hoy.\"\n",
    "  },\n",
    "  {\n",
    "    \"Prompt\": \"\",\n",
    "    \"Answer\": \"\"\n",
    "  },\n",
    "  {\n",
    "    \"Prompt\": \"Explicame lo mejor que puedas la IA con la fisica\",\n",
    "    \"Answer\": \"La inteligencia artificial (IA) es un campo de estudio que se enfoca en desarrollar sistemas y programas capaces de realizar tareas que normalmente requieren de la inteligencia humana. La física, por otro lado, es una disciplina científica que se ocupa del estudio de la naturaleza y las propiedades de la materia y la energía, así como de las interacciones entre ellas.\\n\\nLa IA y la física pueden estar relacionadas de varias maneras. Una de ellas es el uso de la física en la creación de modelos y simulaciones que permiten a los sistemas de IA comprender y predecir el comportamiento de objetos y fenómenos en el mundo real. Por ejemplo, en el campo de la robótica, la física se utiliza para modelar la dinámica de los objetos y las fuerzas que actúan sobre ellos, lo que permite a los robots planificar y ejecutar movimientos precisos.\\n\\nAdemás, la física también puede ser utilizada para mejorar los algoritmos de aprendizaje automático en la IA. Por ejemplo, en el campo del aprendizaje profundo, se pueden aplicar principios físicos para mejorar la eficiencia y la precisión de los modelos de redes neuronales. Esto se logra mediante la incorporación de conocimientos físicos en la estructura y los parámetros de las redes neuronales, lo que permite que los modelos aprendan de manera más rápida y precisa.\\n\\nPor otro lado, la IA también puede contribuir al avance de la física. Los sistemas de IA pueden ser utilizados para analizar grandes cantidades de datos experimentales y teóricos, lo que puede ayudar a los físicos a descubrir patrones y relaciones ocultas en los datos. Además, la IA también puede ser utilizada para simular y predecir el comportamiento de sistemas físicos complejos, lo que puede ayudar a los físicos a diseñar experimentos más eficientes y a comprender mejor los fenómenos físicos.\\n\\nEn resumen, la IA y la física están estrechamente relacionadas y se complementan entre sí. La física proporciona los principios y las leyes fundamentales que permiten a los sistemas de IA comprender y modelar el mundo físico, mientras que la IA puede mejorar la eficiencia y la precisión de los modelos físicos y ayudar a los físicos a descubrir nuevos conocimientos y avances en la disciplina.\"\n",
    "  },\n",
    "  {\n",
    "    \"Prompt\": \"Explica el concepto de relatividad.\",\n",
    "    \"Answer\": \"La inteligencia artificial (IA) es un campo de estudio que se enfoca en desarrollar sistemas y programas capaces de realizar tareas que normalmente requieren de la inteligencia humana. La física, por otro lado, es una disciplina científica que se ocupa del estudio de la naturaleza y las propiedades de la materia y la energía, así como de las interacciones entre ellas.\\n\\nLa IA y la física pueden estar relacionadas de varias maneras. Una de ellas es el uso de la física en la creación de modelos y simulaciones que permiten a los sistemas de IA comprender y predecir el comportamiento de objetos y fenómenos en el mundo real. Por ejemplo, en el campo de la robótica, la física se utiliza para modelar la dinámica de los objetos y las fuerzas que actúan sobre ellos, lo que permite a los robots planificar y ejecutar movimientos precisos.\\n\\nAdemás, la física también puede ser utilizada para mejorar los algoritmos de aprendizaje automático en la IA. Por ejemplo, en el campo del aprendizaje profundo, se pueden aplicar principios físicos para mejorar la eficiencia y la precisión de los modelos de redes neuronales. Esto se logra mediante la incorporación de conocimientos físicos en la estructura y los parámetros de las redes neuronales, lo que permite que los modelos aprendan de manera más rápida y precisa.\\n\\nPor otro lado, la IA también puede contribuir al avance de la física. Los sistemas de IA pueden ser utilizados para analizar grandes cantidades de datos experimentales y teóricos, lo que puede ayudar a los físicos a descubrir patrones y relaciones ocultas en los datos. Además, la IA también puede ser utilizada para simular y predecir el comportamiento de sistemas físicos complejos, lo que puede ayudar a los físicos a diseñar experimentos más eficientes y a comprender mejor los fenómenos físicos.\\n\\nEn resumen, la IA y la física están estrechamente relacionadas y se complementan entre sí. La física proporciona los principios y las leyes fundamentales que permiten a los sistemas de IA comprender y modelar el mundo físico, mientras que la IA puede mejorar la eficiencia y la precisión de los modelos físicos y ayudar a los físicos a descubrir nuevos conocimientos y avances en la disciplina.\"\n",
    "  },\n",
    "  {\n",
    "    \"Prompt\": \"Explica el concepto de relatividad.\",\n",
    "    \"Answer\": \"La inteligencia artificial (IA) es un campo de estudio que busca desarrollar sistemas y programas capaces de realizar tareas que normalmente requieren de la inteligencia humana. La IA utiliza principios y técnicas de la física para mejorar su funcionamiento y eficiencia.\\n\\nEn primer lugar, la física proporciona los fundamentos teóricos necesarios para comprender el mundo que nos rodea. Los principios de la física, como las leyes del movimiento de Newton, la termodinámica y la electromagnetismo, nos permiten entender cómo interactúan los objetos y las fuerzas que actúan sobre ellos. Estos principios son esenciales para desarrollar modelos y algoritmos que permiten a los sistemas de IA comprender y predecir el comportamiento de los objetos en su entorno.\\n\\nAdemás, la física también proporciona herramientas matemáticas y computacionales que son fundamentales para el desarrollo de algoritmos de IA. Por ejemplo, la física utiliza ecuaciones diferenciales para describir el cambio en el tiempo de las variables físicas. Estas ecuaciones diferenciales son utilizadas en la IA para modelar y simular el comportamiento de los sistemas.\\n\\nLa física también es fundamental en el campo de la visión por computadora, que es una rama de la IA que se ocupa de la interpretación de imágenes y videos. La física óptica nos permite entender cómo la luz interactúa con los objetos y cómo se forma una imagen en una cámara. Esto es crucial para el desarrollo de algoritmos de visión por computadora que pueden reconocer objetos, detectar rostros y realizar otras tareas relacionadas con la percepción visual.\\n\\nEn resumen, la física es una disciplina fundamental en el desarrollo de la IA. Proporciona los principios teóricos, las herramientas matemáticas y las técnicas computacionales necesarias para comprender y modelar el mundo que nos rodea. La aplicación de la física en la IA permite a los sistemas y programas de IA comprender y predecir el comportamiento de los objetos en su entorno, lo que les permite realizar tareas de manera más eficiente y precisa.\"\n",
    "  },\n",
    "  {\n",
    "    \"Prompt\": \"Dame más detalle de la segunda Ley de Newton\",\n",
    "    \"Answer\": \"La segunda ley de Newton, también conocida como el principio fundamental de la dinámica, establece que la aceleración de un objeto es directamente proporcional a la fuerza neta aplicada sobre él y está en la misma dirección de esa fuerza. Además, hay una relación inversamente proporcional entre la aceleración y la masa del objeto. Es decir, a mayor fuerza aplicada, mayor aceleración, pero esta aceleración será menor si el objeto tiene mayor masa.\\n\\nLa fórmula asociada con esta ley es *F*=*ma*, donde *F* es la fuerza total que actúa sobre el objeto, *m* es la masa del objeto y *a* es la aceleración que experimenta.\\n\\nEn términos prácticos, esto significa que si empujas un objeto, como un carrito de compras, con una cierta fuerza, la aceleración que adquiere el carrito depende de su masa. Un carrito más pesado (mayor masa) adquirirá menos aceleración que un carrito más ligero con la misma fuerza aplicada.\\n\\nEsta ley también puede expresarse como la variación de la cantidad de movimiento (o momento lineal) en el tiempo, representada por la fórmula *F*=*dt/dp*. Sin embargo, para la mayoría de los contextos, la forma *F*=*ma* es suficientemente precisa y más fácil de usar.\\n\\nEn contextos reales, esta ley tiene numerosas aplicaciones, desde el diseño de vehículos y sistemas de frenado hasta el análisis de la dinámica de fluidos y la ingeniería espacial.\"\n",
    "  },\n",
    "  {\n",
    "    \"Prompt\": \"Cuales son las Leyes de Newton? \",\n",
    "    \"Answer\": \"Las tres leyes de Newton son fundamentales para entender la mecánica clásica. Aquí tienes una explicación conceptual de cada una de ellas, junto con un ejemplo práctico:\\n\\n1. **Primera Ley de Newton o Ley de la Inercia**: Esta ley establece que un objeto permanecerá en reposo o en movimiento rectilíneo uniforme a menos que actúe sobre él una fuerza externa. Conceptualmente, esta ley habla sobre la resistencia de los objetos a cambiar su estado de movimiento.\\n    - **Ejemplo**: Cuando viajas en un automóvil y este frena repentinamente, tiendes a seguir moviéndote hacia adelante. Esto se debe a la inercia, tu cuerpo resiste el cambio en su estado de movimiento.\\n2. **Segunda Ley de Newton**: Esta ley formula que la fuerza aplicada a un objeto es igual a la masa del objeto multiplicada por su aceleración (F = ma). En esencia, esta ley cuantifica cómo la aceleración de un objeto está relacionada con las fuerzas que actúan sobre él y su masa.\\n    - **Ejemplo**: Empujar un carrito de compras vacío requiere menos fuerza que empujar el mismo carrito cuando está lleno de comestibles. Cuanto mayor es la masa del carrito (más comestibles), mayor es la fuerza necesaria para alcanzar la misma aceleración.\\n3. **Tercera Ley de Newton o Ley de Acción y Reacción**: Esta ley afirma que por cada acción, hay una reacción igual y opuesta. En otras palabras, las fuerzas siempre actúan en pares; si un objeto A ejerce una fuerza sobre un objeto B, el objeto B ejerce una fuerza igual y opuesta sobre el objeto A.\\n    - **Ejemplo**: Al saltar desde una lancha hacia un muelle, tu salto ejerce una fuerza hacia abajo sobre la lancha, y la lancha ejerce una fuerza igual y opuesta hacia arriba sobre ti. Esto hace que la lancha se mueva ligeramente hacia atrás cuando saltas.\"\n",
    "  }\n",
    "]\n",
    "history=list(reversed(history))\n",
    "user_message=\"Quien es Tesla?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"This is the story of the chat,lo unico que quiero que respondas es user_message {history} esto es para darte contexto de lo que esta pasando ,this is the question of the user , solo responde esto{user_message}, return the answer of the user\")\n",
    "model = ChatOpenAI(model=\"gpt-4-0125-preview\", temperature=0)\n",
    "chain = prompt | model\n",
    "\n",
    "result=chain.invoke({\"history\":history,\"user_message\":user_message})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from supabase import create_client\n",
    "from decouple import config\n",
    "\n",
    "import os \n",
    "\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts.chat import ChatPromptTemplate\n",
    "\n",
    "from supabase import create_client\n",
    "from decouple import config\n",
    "# Proyecto Admin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_questions(questions_text):\n",
    "    # Separar la cadena de texto en una lista de preguntas usando punto y coma como separador\n",
    "    questions = questions_text.split(';')\n",
    "\n",
    "    # Limpiar espacios en blanco y saltos de línea\n",
    "    questions = [q.strip() for q in questions if q.strip()]\n",
    "\n",
    "    # Convertir cada pregunta en un diccionario con la estructura deseada\n",
    "    followups = [{\"question\": q, \"url\": \"https://ejemplo.com\"} for q in questions]\n",
    "\n",
    "    return {\"followup\": followups}\n",
    "\n",
    "\n",
    "def save_followups_to_db(followups, table_name):\n",
    "    url_user: str = config(\"SUPABASE_USER_URL\")\n",
    "    key_user: str = config(\"SUPABASE_USER_KEY\")\n",
    "    supabase = create_client(url_user, key_user)\n",
    "\n",
    "    # Insertar los datos en la tabla de Supabase\n",
    "    response = supabase.table(table_name).insert(followups).execute()\n",
    "    print(response)\n",
    "\n",
    "def run_facts(query):\n",
    "    prompt_template = ChatPromptTemplate.from_template(    \"\"\"\n",
    "    Tu trabajo es darme una lista de preguntas relacionadas a {query}.\n",
    "    Por favor, devuelve las preguntas en formato de lista separadas por punto y coma (;) de esta manera:\n",
    "    Pregunta1;Pregunta2;Pregunta3\n",
    "    \"\"\")\n",
    "    model = ChatOpenAI(model=\"gpt-3.5-turbo-1106\", temperature=0)\n",
    "    chain = prompt_template | model\n",
    "\n",
    "    result = chain.invoke({\"query\": query})\n",
    "    print(result.content)\n",
    "\n",
    "    followups = process_questions(result.content)\n",
    "    # save_followups_to_db(followups, \"your_table_name\")\n",
    "\n",
    "    return followups\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=run_facts(\"Quien es isaac Newton?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multi_agents.videos import VideosQA\n",
    "\n",
    "videos=VideosQA(courseid=\"547fec61-e5ee-450d-9e35-ac4f26b2b02d\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos.query(query_text=\"Que es un activo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multi_agents.sources import SourcesQA\n",
    "\n",
    "sources=SourcesQA(courseid=\"547fec61-e5ee-450d-9e35-ac4f26b2b02d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources.query(query_text=\"Que es un activo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decouple import config\n",
    "from supabase import create_client\n",
    "url_user: str = config(\"SUPABASE_ADMIN_URL\")\n",
    "key_user: str = config(\"SUPABASE_ADMIN_KEY\")\n",
    "supabase_user = create_client(supabase_url=url_user,supabase_key= key_user)\n",
    "thread_id=\"a0ea4f43-85a5-4a43-8ed3-c5e27c67203d\"\n",
    "courseid=\"73e5618f-e2c0-476e-87d3-43d352f14ee0\"\n",
    "\n",
    "thread_exists = supabase_user.storage.list_buckets()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thread_exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = \"CoursesFiles\"\n",
    "carpeta=\"a230d7d1-7239-4627-8190-1a33b686ae66\"\n",
    "# Listar archivos en el bucket\n",
    "files = supabase_user.storage.from_(bucket_name).list(carpeta)\n",
    "print(files)\n",
    "\n",
    "for file in files:\n",
    "    print(file[\"name\"])  # O cualquier otro atributo relevante\n",
    "    \n",
    "    print(supabase_user.storage.from_(bucket_name).get_public_url(f'{carpeta}/{file[\"name\"]}'))\n",
    "    print(file[\"metadata\"][\"mimetype\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "import tempfile\n",
    "import os\n",
    "from langchain.schema import Document\n",
    "from langchain_community.vectorstores import DeepLake\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from decouple import config\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = config(\"OPENAI_API_KEY\")\n",
    "os.environ[\"ACTIVELOOP_TOKEN\"] = config(\"ACTIVELOOP_TOKEN\")\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "username_or_org = \"<USERNAME_OR_ORG>\"\n",
    "\n",
    "courseid=\"0e20d84a-f013-4b80-82bc-628b61aa6b7f\"\n",
    "for file in files:\n",
    "    if file[\"metadata\"][\"mimetype\"]==\"application/pdf\":\n",
    "      file_path = f\"{carpeta}/{file['name']}\"\n",
    "      print(f\"Descargando archivo: {file_path}\")\n",
    "\n",
    "      # Descargar el contenido del archivo\n",
    "      response = supabase_user.storage.from_(bucket_name).download(file_path)\n",
    "      # Crear un directorio temporal\n",
    "      temp_dir = tempfile.mkdtemp()\n",
    "      temp_file_path = os.path.join(temp_dir, file['name'])\n",
    "\n",
    "      # Guardar el contenido descargado en un archivo dentro del directorio temporal\n",
    "      with open(temp_file_path, 'wb') as temp_file:\n",
    "          temp_file.write(response)\n",
    "\n",
    "      print(f\"Archivo temporal creado: {temp_file_path}\")\n",
    "\n",
    "      # Cargar el archivo PDF desde la ruta temporal y procesarlo\n",
    "      loader = PyPDFLoader(temp_file.name)\n",
    "      pages = loader.load_and_split()\n",
    "      print(pages)\n",
    "      vectorstore = DeepLake.from_documents(\n",
    "      pages,\n",
    "      embeddings,\n",
    "      dataset_path=f\"hub://skillstech/PDF-{courseid}\",\n",
    "  )\n",
    "\n",
    "      # Opcional: eliminar el archivo temporal después de procesarlo\n",
    "      os.unlink(temp_file.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multi_agents.sources import SourcesQA\n",
    "prompt=\"Quiero saber sobre fisica de particulas , que podemos saber de esto , que es la velocidad media ??\"\n",
    "sources=SourcesQA(courseid=courseid,id=id)\n",
    "result=sources.query(query_text=prompt)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multi_agents.videos import VideosQA\n",
    "prompt=\"Quiero saber sobre fisica de particulas , que podemos saber de esto , que es la velocidad media ??\"\n",
    "sources=VideosQA(courseid=courseid,id=id)\n",
    "result=sources.query(query_text=prompt)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source=result['source_documents'][0].metadata['source']\n",
    "page=result['source_documents'][0].metadata['page']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "nombre_libro_regex = re.search(r'/([^/]*)$', source).group(1)\n",
    "nombre_libro_regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources=[]\n",
    "i=1\n",
    "carpeta=\"2e2aec92-0980-483e-9edd-05f187d10253\"\n",
    "\n",
    "\n",
    "\n",
    "for results in result[\"source_documents\"]:\n",
    "    source=result['source_documents'][0].metadata['source']\n",
    "    nombre_libro_regex = re.search(r'/([^/]*)$', source).group(1)\n",
    "\n",
    "    page=result['source_documents'][0].metadata['page']\n",
    "    url=supabase_user.storage.from_(bucket_name).get_public_url(f'{carpeta}/{nombre_libro_regex}')\n",
    "    sources.append({\n",
    "        \"url\":f\"{url}#page={page}\",\n",
    "        \"title\":results.page_content,\n",
    "        \"sourceNumber\": i}\n",
    "        )\n",
    "    i+=1\n",
    "    \n",
    "data={\"sources\":sources}\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "  \"sources\": [\n",
    "    {\n",
    "      \"url\": \"https://www.sciencedaily.com\",\n",
    "      \"title\": \"Science Daily - Latest Science News\",\n",
    "      \"sourceNumber\": 1\n",
    "    },\n",
    "    {\n",
    "      \"url\": \"https://techcrunch.com\",\n",
    "      \"title\": \"TechCrunch - Technology News\",\n",
    "      \"sourceNumber\": 2\n",
    "    },\n",
    "    {\n",
    "      \"url\": \"https://www.nationalgeographic.com\",\n",
    "      \"title\": \"National Geographic - Explore Nature\",\n",
    "      \"sourceNumber\": 3\n",
    "    }\n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=sources.query(query_text=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"Hola\"\n",
    "response_data = { \"sources\":[{\n",
    "        \"url\":\"www.google.com\",\n",
    "        \"title\":text\n",
    "        }\n",
    "        ],\n",
    "    }\n",
    "\n",
    "thread_exists = supabase_user.table(\"responses_tb\").update({\"sources\":response_data}).eq(\"id\", 115).execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thread_exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yt_dlp\n",
    "from langchain.document_loaders import AssemblyAIAudioTranscriptLoader\n",
    "from langchain_community.vectorstores import DeepLake\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "import yt_dlp\n",
    "from yt_dlp.utils import DownloadError\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders.assemblyai import TranscriptFormat\n",
    "\n",
    "from langchain.document_loaders import AssemblyAIAudioTranscriptLoader\n",
    "from langchain_community.vectorstores import DeepLake\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "import assemblyai as aai\n",
    "aai.settings.api_key = \"26f195ae63cf434280dd530fb61d6981\"\n",
    "\n",
    "import json\n",
    "\n",
    "from decouple import config\n",
    "from supabase import create_client\n",
    "\n",
    "url_user: str = config(\"SUPABASE_USER_URL\")\n",
    "key_user: str = config(\"SUPABASE_USER_KEY\")\n",
    "supabase_user = create_client(supabase_url=url_user,supabase_key= key_user)\n",
    "\n",
    "\n",
    "class YouTubeTranscription:\n",
    "    def __init__(self, course_id=None):\n",
    "        self.course_id = course_id\n",
    "        self.embeddings = OpenAIEmbeddings()\n",
    "        \n",
    "        url_admin: str = config(\"SUPABASE_ADMIN_URL\")\n",
    "        key_admin: str = config(\"SUPABASE_ADMIN_KEY\")\n",
    "        self.supabase = create_client(supabase_url=url_admin,supabase_key= key_admin)\n",
    "\n",
    "\n",
    "    def get_transcript_yt(self, YT_URL):\n",
    "        try:\n",
    "            with yt_dlp.YoutubeDL() as ydl:\n",
    "                info = ydl.extract_info(YT_URL, download=False)\n",
    "            if \"entries\" in info:\n",
    "                info = info[\"entries\"][0]\n",
    "            YT_title = info.get('title', None)\n",
    "\n",
    "            if \"formats\" not in info:\n",
    "                print(f\"Formats key not found in info dictionary for video {YT_URL}. Skipping...\")\n",
    "                return None, None, None\n",
    "\n",
    "            audio_url = None\n",
    "            for format in info[\"formats\"][::-1]:\n",
    "                if format[\"acodec\"] != \"none\":\n",
    "                    audio_url = format[\"url\"]\n",
    "                    break\n",
    "\n",
    "            return YT_URL, YT_title, audio_url\n",
    "        except DownloadError as e:\n",
    "            print(f\"Error downloading video {YT_URL}: {e}\")\n",
    "            return None, None, None\n",
    "        \n",
    "    def url_to_docs(self, YT_URL, YT_title, audio_url):\n",
    "        config = aai.TranscriptionConfig(\n",
    "                language_detection=True,\n",
    "                #auto_highlights=True,\n",
    "                #summarization=True, summarization incompatible with auto_chapters\n",
    "                #auto_chapters=True,\n",
    "                )\n",
    "        loader = AssemblyAIAudioTranscriptLoader(audio_url,config=config,transcript_format=TranscriptFormat.PARAGRAPHS,)\n",
    "        docs = loader.load()\n",
    "        for doc in docs:\n",
    "                doc.metadata = {\"source\": YT_URL, \"title\": YT_title, \"start\":doc.metadata[\"start\"], \"end\":doc.metadata[\"end\"]}\n",
    "        return docs\n",
    "\n",
    "    def docs_to_deeplakeDB(self, docs,course_id):\n",
    "        \n",
    "        dataset_path = f\"hub://skillstech/VIDEO-{self.course_id}\" if self.course_id else \"default_path\"\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
    "        texts=[]\n",
    "        for document in docs:\n",
    "            texts.extend(text_splitter.split_documents(document))\n",
    "            print(texts)\n",
    "                # Convertir la lista a una única cadena JSON\n",
    "            documents_str = '\\n'.join([json.dumps(doc, indent=None, default=str) for doc in texts])\n",
    "\n",
    "        self.supabase.table(\"courses_tb\").update({\"video_docs_vdb\": documents_str}).eq(\"id\", course_id).execute()\n",
    "\n",
    "        vectorstore = DeepLake(dataset_path=dataset_path, embedding=self.embeddings, overwrite=True)\n",
    "        vectorstore.add_documents(texts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from supabase import create_client\n",
    "from decouple import config\n",
    "class CourseVideoProcessor:\n",
    "    def __init__(self):\n",
    "        url_admin: str = config(\"SUPABASE_ADMIN_URL\")\n",
    "        key_admin: str = config(\"SUPABASE_ADMIN_KEY\")\n",
    "        self.lista_de_docs=[]\n",
    "\n",
    "\n",
    "        self.supabase = create_client(supabase_url=url_admin,supabase_key= key_admin)\n",
    "\n",
    "    def process_all_courses(self):\n",
    "        courses_data = self.supabase.table(\"courses_tb\").select(\"*\").execute().data\n",
    "        for course in courses_data:\n",
    "            if course['reference_videos'] and course['video_processed'] != 'TRUE':\n",
    "                self.transcriber = YouTubeTranscription(course_id=course['id'])\n",
    "                print(\"Este es el curso\",course['id'])\n",
    "                for video_url in course['reference_videos']:\n",
    "                    if video_url:  # Asegurar que la URL no está vacía\n",
    "                        URL, title, audio_url = self.transcriber.get_transcript_yt(video_url)\n",
    "                        if URL and title and audio_url:  # Asegurar que todos los componentes son válidos\n",
    "                            docs = self.transcriber.url_to_docs(URL, title, audio_url)\n",
    "                            self.lista_de_docs.append(docs)\n",
    "                print(\"😍😍😍😍😍😍😍😍,esta es la lista de docs\",self.lista_de_docs)\n",
    "                self.transcriber.docs_to_deeplakeDB(self.lista_de_docs,course_id=course['id'])\n",
    "                self.supabase.table(\"courses_tb\").update({\"video_processed\": \"TRUE\"}).eq(\"id\", course['id']).execute()\n",
    "                self.lista_de_docs=[]\n",
    "\n",
    "processor = CourseVideoProcessor()\n",
    "processor.process_all_courses()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Este es el curso f7920bf1-89de-4f66-be4a-3973a5c48fdd\n",
    "\n",
    "# Dataset(path='hub://skillstech/VIDEO-1f0319a2-0d35-4a77-b20e-a2b8bcedc28d', tensors=['text', 'metadata', 'embedding', 'id'])\n",
    "\n",
    "# Uso de la clase\n",
    "youtube_urls = [\"https://www.youtube.com/watch?v=lOC7aKQlOx8\", \"https://www.youtube.com/watch?v=xxE_iVf5_gI\"]\n",
    "course_id = \"73e5618f-e2c0-476e-87d3-43d352f14ee0\"\n",
    "transcriber = YouTubeTranscription(course_id)\n",
    "\n",
    "for yt_url in youtube_urls:\n",
    "    URL, title, audio_url = transcriber.get_transcript_yt(yt_url)\n",
    "    print(\"TITLE:\", title)\n",
    "    docs = transcriber.url_to_docs(URL, title, audio_url)\n",
    "    transcriber.docs_to_deeplakeDB(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from supabase import create_client\n",
    "from decouple import config\n",
    "import tempfile\n",
    "import os\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.schema import Document\n",
    "from langchain_community.vectorstores import DeepLake\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "class CourseProcessor:\n",
    "    def __init__(self):\n",
    "        os.environ[\"OPENAI_API_KEY\"] = config(\"OPENAI_API_KEY\")\n",
    "        os.environ[\"ACTIVELOOP_TOKEN\"] = config(\"ACTIVELOOP_TOKEN\")\n",
    "\n",
    "        self.embeddings = OpenAIEmbeddings()\n",
    "\n",
    "        url_admin = config(\"SUPABASE_ADMIN_URL\")\n",
    "        key_admin = config(\"SUPABASE_ADMIN_KEY\")\n",
    "        self.supabase = create_client(supabase_url=url_admin, supabase_key=key_admin)\n",
    "\n",
    "        self.successful_courses = []\n",
    "        self.failed_courses = []\n",
    "\n",
    "    def process_courses(self):\n",
    "        course_list = self.supabase.table(\"courses_tb\").select(\"*\").execute().data\n",
    "\n",
    "        for course in course_list:\n",
    "            try:\n",
    "                self.process_course(course)\n",
    "                self.successful_courses.append(f\"{course['name']} ({course['id']})\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error al procesar el curso {course['name']} ({course['id']}): {str(e)}\")\n",
    "                self.failed_courses.append(f\"{course['name']} ({course['id']}): {str(e)}\")\n",
    "\n",
    "        self.generate_report()\n",
    "\n",
    "    # def process_course(self, course):\n",
    "    #     reference_files = course[\"reference_files\"]\n",
    "    #     courseid = course[\"id\"]\n",
    "\n",
    "    #     if reference_files and isinstance(reference_files, list) and course['pdf_processed'] != 'TRUE':\n",
    "    #         for ref_file in reference_files:\n",
    "    #             for _, value in ref_file.items():\n",
    "    #                 self.download_and_process_file(value, courseid)\n",
    "    #     else:\n",
    "    #         print(f\"No hay archivos de referencia para el curso {course['name']} ({course['id']})\")\n",
    "\n",
    "    # def download_and_process_file(self, file_path, courseid):\n",
    "    #     folder_name, file_name = file_path.rsplit('/', 1)\n",
    "    #     response = self.supabase.storage.from_(\"CoursesFiles\").download(file_path)\n",
    "    #     temp_dir = tempfile.mkdtemp()\n",
    "    #     temp_file_path = os.path.join(temp_dir, file_name)\n",
    "\n",
    "    #     with open(temp_file_path, 'wb') as temp_file:\n",
    "    #         temp_file.write(response)\n",
    "\n",
    "    #     self.process_pdf(temp_file_path, courseid)\n",
    "    #     os.unlink(temp_file_path)\n",
    "    def process_course(self, course):\n",
    "        reference_files = course[\"reference_files\"]\n",
    "        courseid = course[\"id\"]\n",
    "\n",
    "        if reference_files and isinstance(reference_files, list) and course['status'] != 'ready':\n",
    "            for ref_file in reference_files:\n",
    "                url = ref_file[\"url\"]\n",
    "                name = ref_file[\"name\"]\n",
    "                self.download_and_process_file(url, name, courseid)\n",
    "        else:\n",
    "            print(f\"No hay archivos de referencia para el curso {course['name']} ({course['id']})\")\n",
    "\n",
    "    def download_and_process_file(self, file_url, file_name, courseid):\n",
    "        response = self.supabase.storage.from_(\"CoursesFiles\").download(file_url)\n",
    "        temp_dir = tempfile.mkdtemp()\n",
    "        temp_file_path = os.path.join(temp_dir, file_name)\n",
    "\n",
    "        with open(temp_file_path, 'wb') as temp_file:\n",
    "            temp_file.write(response)\n",
    "\n",
    "        self.process_pdf(temp_file_path, courseid)\n",
    "        os.unlink(temp_file_path)\n",
    "\n",
    "    def process_pdf(self, file_path, courseid):\n",
    "        loader = PyPDFLoader(file_path)\n",
    "        pages = loader.load_and_split()\n",
    "        DeepLake.from_documents(pages, self.embeddings, dataset_path=f\"hub://skillstech/PDF-{courseid}\",overwrite=False)\n",
    "        print(\"😍😍😍😍😍😍😍😍😍😍😍😍😍😍😍😍😍😍 SE ESTA CAMBIANDO A TRU PDF\",courseid)\n",
    "        save=self.supabase.table(\"courses_tb\").update({\"pdf_processed\": \"TRUE\"}).eq(\"id\", courseid).execute()\n",
    "        print(save)\n",
    "        print(\"😍😍😍😍😍😍😍😍😍😍😍😍😍😍😍😍😍😍  SE CAMBIO A  TRUE PDF\")\n",
    "\n",
    "    def generate_report(self):\n",
    "        print(\"Reporte de procesamiento de cursos:\")\n",
    "        print(\"Cursos procesados exitosamente:\")\n",
    "        for course in self.successful_courses:\n",
    "            print(course)\n",
    "\n",
    "        print(\"\\nCursos que fallaron al procesarse:\")\n",
    "        for course in self.failed_courses:\n",
    "            print(course)\n",
    "\n",
    "# Uso de la clase\n",
    "processor = CourseProcessor()\n",
    "processor.process_courses()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "import tempfile\n",
    "import os\n",
    "from langchain.schema import Document\n",
    "from langchain_community.vectorstores import DeepLake\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from decouple import config\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = config(\"OPENAI_API_KEY\")\n",
    "os.environ[\"ACTIVELOOP_TOKEN\"] = config(\"ACTIVELOOP_TOKEN\")\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "username_or_org = \"<USERNAME_OR_ORG>\"\n",
    "\n",
    "courseid=\"73e5618f-e2c0-476e-87d3-43d352f14ee0\"\n",
    "for file in files:\n",
    "    if file[\"metadata\"][\"mimetype\"]==\"application/pdf\":\n",
    "      file_path = f\"{carpeta}/{file['name']}\"\n",
    "      print(f\"Descargando archivo: {file_path}\")\n",
    "\n",
    "      # Descargar el contenido del archivo\n",
    "      response = supabase_user.storage.from_(bucket_name).download(file_path)\n",
    "      # Crear un directorio temporal\n",
    "      temp_dir = tempfile.mkdtemp()\n",
    "      temp_file_path = os.path.join(temp_dir, file['name'])\n",
    "\n",
    "      # Guardar el contenido descargado en un archivo dentro del directorio temporal\n",
    "      with open(temp_file_path, 'wb') as temp_file:\n",
    "          temp_file.write(response)\n",
    "\n",
    "      print(f\"Archivo temporal creado: {temp_file_path}\")\n",
    "\n",
    "      # Cargar el archivo PDF desde la ruta temporal y procesarlo\n",
    "      loader = PyPDFLoader(temp_file.name)\n",
    "      pages = loader.load_and_split()\n",
    "      print(pages)\n",
    "      vectorstore = DeepLake.from_documents(\n",
    "      pages,\n",
    "      embeddings,\n",
    "      dataset_path=f\"hub://skillstech/PDF-{courseid}\",\n",
    "  )\n",
    "\n",
    "      # Opcional: eliminar el archivo temporal después de procesarlo\n",
    "      os.unlink(temp_file.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decouple import config\n",
    "from supabase import create_client\n",
    "\n",
    "\n",
    "url_admin = config(\"SUPABASE_ADMIN_URL\")\n",
    "key_admin = config(\"SUPABASE_ADMIN_KEY\")\n",
    "supabase_admin = create_client(supabase_url=url_admin, supabase_key=key_admin)\n",
    "\n",
    "data_course=supabase_admin.table(\"courses_tb\").select(\"*\").eq(\"id\",\"7bc9e9e7-f1d2-4967-ad27-fd1697885f0b\").execute().data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_course[0]['companyid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from celery_worker import app\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decouple import config\n",
    "from supabase import create_client\n",
    "\n",
    "\n",
    "courseid=\"73e5618f-e2c0-476e-87d3-43d352f14ee0\"\n",
    "\n",
    "url_admin = config(\"SUPABASE_ADMIN_URL\")\n",
    "key_admin = config(\"SUPABASE_ADMIN_KEY\")\n",
    "supabase_admin = create_client(supabase_url=url_admin, supabase_key=key_admin)\n",
    "\n",
    "data_course=supabase_admin.table(\"courses_tb\").select(\"*\").eq(\"id\",courseid).execute().data\n",
    "\n",
    "carpeta = data_course[0]['companyid']\n",
    "\n",
    "print(data_course)\n",
    "print(carpeta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = \"CoursesFiles\"\n",
    "\n",
    "url = supabase_admin.storage.from_(bucket_name).get_public_url(f'{carpeta}/Seller_Academy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_admin: str = config(\"SUPABASE_ADMIN_URL\")\n",
    "key_admin: str = config(\"SUPABASE_ADMIN_KEY\")\n",
    "supabase = create_client(supabase_url=url_admin,supabase_key= key_admin)\n",
    "course={}\n",
    "course['id']=\"890e55ce-825b-4b24-a68f-4593f98c74fe\"\n",
    "\n",
    "video=supabase.table(\"courses_tb\").update({\"video_processed\": \"True\"}).eq(\"id\", course['id']).execute()\n",
    "video_status=supabase.table(\"courses_tb\").select(\"video_processed\").eq(\"id\", course['id']).execute()\n",
    "\n",
    "status=supabase.table(\"courses_tb\").update({\"status\": \"ready\"}).eq(\"id\", course['id']).execute()\n",
    "print(video_status)\n",
    "print(video)\n",
    "print(status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from supabase import create_client\n",
    "from decouple import config\n",
    "\n",
    "url_admin: str = config(\"SUPABASE_ADMIN_URL\")\n",
    "key_admin: str = config(\"SUPABASE_ADMIN_KEY\")\n",
    "supabase = create_client(supabase_url=url_admin, supabase_key=key_admin)\n",
    "course_id = \"890e55ce-825b-4b24-a68f-4593f98c74fe\"\n",
    "\n",
    "# Update video_processed field\n",
    "video_response = supabase.table(\"courses_tb\").update({\"video_processed\": \"false\"}).eq(\"id\", course_id).execute()\n",
    "print(f\"Video Update Response: {video_response}\")\n",
    "\n",
    "# Fetch and print video_processed status\n",
    "video_status = supabase.table(\"courses_tb\").select(\"video_processed\").eq(\"id\", course_id).execute()\n",
    "print(f\"Video Status Response: {video_status}\")\n",
    "\n",
    "# Update status field\n",
    "status_response = supabase.table(\"courses_tb\").update({\"status\": \"ready\"}).eq(\"id\", course_id).execute()\n",
    "print(f\"Status Update Response: {status_response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from supabase import create_client\n",
    "from decouple import config\n",
    "import tempfile\n",
    "import os\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.schema import Document\n",
    "from langchain_community.vectorstores import DeepLake\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "class CourseProcessor:\n",
    "    def __init__(self):\n",
    "        os.environ[\"OPENAI_API_KEY\"] = config(\"OPENAI_API_KEY\")\n",
    "        os.environ[\"ACTIVELOOP_TOKEN\"] = config(\"ACTIVELOOP_TOKEN\")\n",
    "\n",
    "        self.embeddings = OpenAIEmbeddings()\n",
    "\n",
    "        url_admin = config(\"SUPABASE_ADMIN_URL\")\n",
    "        key_admin = config(\"SUPABASE_ADMIN_KEY\")\n",
    "        self.supabase = create_client(supabase_url=url_admin, supabase_key=key_admin)\n",
    "\n",
    "        self.successful_courses = []\n",
    "        self.failed_courses = []\n",
    "\n",
    "    def process_courses(self):\n",
    "        course_list = self.supabase.table(\"courses_tb\").select(\"*\").execute().data\n",
    "\n",
    "        for course in course_list:\n",
    "            try:\n",
    "                self.process_course(course)\n",
    "                self.successful_courses.append(f\"{course['name']} ({course['id']})\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error al procesar el curso {course['name']} ({course['id']}): {str(e)}\")\n",
    "                self.failed_courses.append(f\"{course['name']} ({course['id']}): {str(e)}\")\n",
    "\n",
    "        self.generate_report()\n",
    "\n",
    "    def process_course(self, course):\n",
    "        reference_files = course[\"reference_files\"]\n",
    "        courseid = course[\"id\"]\n",
    "\n",
    "        if reference_files and isinstance(reference_files, list) and course['pdf_processed'] != 'TRUE':\n",
    "            for ref_file in reference_files:\n",
    "                url = ref_file[\"url\"]\n",
    "                name = ref_file[\"name\"]\n",
    "                self.download_and_process_file(file_url=url, file_name=name, courseid=courseid)\n",
    "        else:\n",
    "            print(f\"No hay archivos de referencia para el curso {course['name']} ({course['id']})\")\n",
    "\n",
    "    def download_and_process_file(self, file_url, file_name, courseid):\n",
    "        response = self.supabase.storage.from_(\"CoursesFiles\").download(file_url)\n",
    "        temp_dir = tempfile.mkdtemp()\n",
    "        temp_file_path = os.path.join(temp_dir, file_name)\n",
    "\n",
    "        with open(temp_file_path, 'wb') as temp_file:\n",
    "            temp_file.write(response)\n",
    "\n",
    "        self.process_pdf(temp_file_path, courseid)\n",
    "        os.unlink(temp_file_path)\n",
    "\n",
    "\n",
    "    def process_pdf(self, file_path, courseid):\n",
    "        loader = PyPDFLoader(file_path)\n",
    "        pages = loader.load_and_split()\n",
    "        DeepLake.from_documents(pages, self.embeddings, dataset_path=f\"hub://skillstech/PDF-{courseid}\",overwrite=True)\n",
    "        print(\"😍😍😍😍😍😍😍😍😍😍😍😍😍😍😍😍😍😍 SE ESTA CAMBIANDO A TRU PDF\",courseid)\n",
    "        save=self.supabase.table(\"courses_tb\").update({\"pdf_processed\": \"TRUE\"}).eq(\"id\", courseid).execute()\n",
    "        print(save)\n",
    "        print(\"😍😍😍😍😍😍😍😍😍😍😍😍😍😍😍😍😍😍  SE CAMBIO A  TRUE PDF\")\n",
    "\n",
    "    def generate_report(self):\n",
    "        print(\"Reporte de procesamiento de cursos:\")\n",
    "        print(\"Cursos procesados exitosamente:\")\n",
    "        for course in self.successful_courses:\n",
    "            print(course)\n",
    "\n",
    "        print(\"\\nCursos que fallaron al procesarse:\")\n",
    "        for course in self.failed_courses:\n",
    "            print(course)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CourseProcessor().process_courses()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multi_agents.videos import VideosQA\n",
    "from multi_agents.sources import SourcesQA\n",
    "\n",
    "\n",
    "import asyncio\n",
    "loop = asyncio.get_event_loop()\n",
    "\n",
    "courseid=\"0e20d84a-f013-4b80-82bc-628b61aa6b7f\"\n",
    "thread_id= \"ae24d088-0cb2-4696-b220-d29b31904b36\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos = VideosQA(courseid=courseid, id=id,thread_id=thread_id,first_response=\"Firt response\")\n",
    "sources = SourcesQA(courseid=courseid, id=id)\n",
    "\n",
    "# Envía las tareas en segundo plano y continúa sin esperar a que finalicen\n",
    "video_task = await asyncio.create_task(videos.query(prompt))\n",
    "source_task = await asyncio.create_task(sources.query(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from celery_functions.task import CourseProcessor\n",
    "\n",
    "CourseProcessor().update_pdf()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from celery_functions.task import CourseVideoProcessor\n",
    "CourseVideoProcessor().update_all_courses()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from celery_functions.task import CourseFactsProcessor\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "loader = TextLoader(\"subir_info.txt\")\n",
    "documents = loader.load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CourseFactsProcessor().vector_pdf_database(courseid=\"547fec61-e5ee-450d-9e35-ac4f26b2b02d\",pages=docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multi_agents.sources import SourcesQA\n",
    "import asyncio\n",
    "\n",
    "sources = SourcesQA(courseid=\"23436f47-f4c4-4c6b-bb5e-c5e12ea0c43a\", id=\"935\",orgid=\"781ea14a-b753-4635-982c-a6e0d64aafb1\")\n",
    "\n",
    "# Envía las tareas en segundo plano y continúa sin esperar a que finalicen\n",
    "source_task = await asyncio.create_task(sources.query(\"Bolsa mexicana de valores\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analisis Threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Gemini\n",
    "import os\n",
    "import google.generativeai as genai\n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyDqRltPWDD4-HUxSJ9FzkEuCQ3T1F2lqKg\"\n",
    "\n",
    "\n",
    "### Supabase\n",
    "from supabase import create_client\n",
    "import json\n",
    "\n",
    "# Obtain URL and KEY from project settings (see image above)\n",
    "#SUPABASE_URL='https://rmrxypbhohmkiihdpkqz.supabase.co' # JV test\n",
    "SUPABASE_URL='https://hhtrmdrpowaaxlydfwjn.supabase.co'\n",
    "# Needs service_role key for inserting values to table\n",
    "SUPABASE_KEY='eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6ImhodHJtZHJwb3dhYXhseWRmd2puIiwicm9sZSI6InNlcnZpY2Vfcm9sZSIsImlhdCI6MTcwNTM1OTYzNywiZXhwIjoyMDIwOTM1NjM3fQ.zQDiHYi2DRpD8gyYmVuJxR-yqTmF2xYVLJ737ECqryk'\n",
    "supabase = create_client(SUPABASE_URL, SUPABASE_KEY)\n",
    "\n",
    "#Check https://supabase.com/docs/reference/python/installing for supabase-py options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Gemini\n",
    "import os\n",
    "import google.generativeai as genai\n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyDqRltPWDD4-HUxSJ9FzkEuCQ3T1F2lqKg\"\n",
    "\n",
    "\n",
    "### Supabase\n",
    "from supabase import create_client\n",
    "import json\n",
    "\n",
    "# Obtain URL and KEY from project settings (see image above)\n",
    "#SUPABASE_URL='https://rmrxypbhohmkiihdpkqz.supabase.co' # JV test\n",
    "SUPABASE_URL='https://hhtrmdrpowaaxlydfwjn.supabase.co'\n",
    "# Needs service_role key for inserting values to table\n",
    "SUPABASE_KEY='eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6ImhodHJtZHJwb3dhYXhseWRmd2puIiwicm9sZSI6InNlcnZpY2Vfcm9sZSIsImlhdCI6MTcwNTM1OTYzNywiZXhwIjoyMDIwOTM1NjM3fQ.zQDiHYi2DRpD8gyYmVuJxR-yqTmF2xYVLJ737ECqryk'\n",
    "supabase = create_client(SUPABASE_URL, SUPABASE_KEY)\n",
    "\n",
    "#Check https://supabase.com/docs/reference/python/installing for supabase-py options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyDqRltPWDD4-HUxSJ9FzkEuCQ3T1F2lqKg\"\n",
    "\n",
    "def reconstruct_conversation(thread_id):\n",
    "    \"\"\"\n",
    "    Takes a Supabase table and reconstructs conversation stored in columns\n",
    "    'prompt' and 'answer'\n",
    "    \"\"\"\n",
    "    # Execute the query\n",
    "    result = supabase.table('responses_tb')\\\n",
    "        .select('*')\\\n",
    "        .eq('threadid', thread_id)\\\n",
    "        .execute()\n",
    "\n",
    "    # Directly accessing the data from the result\n",
    "    data = result.data  # Adjusted based on APIResponse object properties\n",
    "\n",
    "    # Verify that data is not None or empty\n",
    "    if not data:\n",
    "        print(\"No data found.\")\n",
    "        return\n",
    "\n",
    "    # If there's an error (assuming result.error exists and is meaningful)\n",
    "    if hasattr(result, 'error') and result.error:\n",
    "        print(f\"An error occurred: {result.error}\")\n",
    "        return\n",
    "\n",
    "    # Initialize a variable to accumulate the conversation\n",
    "    conversation = \"\"\n",
    "\n",
    "    # Iterate through the data to reconstruct the conversation\n",
    "    for item in data:\n",
    "        prompt = item['prompt'] if 'prompt' in item else 'No prompt provided'\n",
    "        answer = item['answer'] if 'answer' in item else 'No answer provided'\n",
    "        conversation += f\"prompt: {prompt}\\nanswer: {answer}\\n\\n\"  # Add extra newline for separation\n",
    "\n",
    "    return conversation.strip()\n",
    "\n",
    "\n",
    "def analyze_conversation(conversation, prompt):\n",
    "    \"\"\"\n",
    "    Summarize conversation using Gemini and a prompt\n",
    "    \"\"\"\n",
    "    # Initialize the language model\n",
    "    llm = ChatGoogleGenerativeAI(model=\"gemini-pro\")\n",
    "\n",
    "    # Invoke the chain with the provided conversation\n",
    "    chain = prompt | llm\n",
    "    result = chain.invoke({\"conversation\": conversation})\n",
    "\n",
    "    # Instead of printing, save the result to a variable\n",
    "    conversationAnalysis = result.content  # Adjust based on actual attribute/method to access the text\n",
    "\n",
    "    # Return the summary text\n",
    "    return conversationAnalysis\n",
    "\n",
    "\n",
    "def update_thread_info(thread_id, summary, metrics):\n",
    "    \"\"\"\n",
    "    Updates the thread summary and metrics for an existing record in 'threads_tb'.\n",
    "\n",
    "    :param thread_id: The ID of the thread whose summary and metrics are to be updated.\n",
    "    :param summary: The new thread summary text (variable AA).\n",
    "    :param metrics: The new thread metrics text (variable BB).\n",
    "\n",
    "    \"\"\"\n",
    "    current_utc_time = datetime.now(pytz.utc).isoformat()\n",
    "\n",
    "    # Assuming 'id' is the column name for the thread ID in 'threads_tb'\n",
    "    response = supabase.table('threads_tb').update({\n",
    "        'thread_summary': summary,\n",
    "        'thread_metrics': metrics,\n",
    "        'last_updated_analysis': current_utc_time\n",
    "\n",
    "    }).eq('id', thread_id).execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from supabase import create_client\n",
    "from decouple import config\n",
    "\n",
    "url_supabase = config('SUPABASE_USER_URL')\n",
    "key_supabase = config('SUPABASE_USER_KEY')\n",
    "supabase = create_client(url_supabase, key_supabase)\n",
    "\n",
    "thread_id = supabase.table('threads_tb').select(\"*\").execute().data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thread_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threads_list=[]\n",
    "for thread in thread_id:\n",
    "    threads_list.append(thread[\"id\"])\n",
    "print(threads_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get summary and metrics for threads_tb\n",
    "\n",
    "promptSummary = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Analyze the following conversation thread as a whole and give me an\n",
    "    executive summary of the interaction to be used for evaluating engagement.\n",
    "    {conversation}\n",
    "\n",
    "    You MUST include the summary with the following elements:\n",
    "    - Topic\n",
    "    - Description of the thread\n",
    "    - Level of engagement\n",
    "    - Comments\n",
    "\n",
    "    ALWAYS give me the answer as a list in English\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "promptMetrics = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Do an exhaustive analysis of the following conversation thread and return\n",
    "    metrics relevant for evaluating usage. The conversation is in the format\n",
    "    prompt -> answer:\n",
    "    {conversation}\n",
    "          \n",
    "      - Number of prompts answered\n",
    "      - Number of positive words\n",
    "      - Comma-separated list of positive words\n",
    "      - Number of negative words\n",
    "      - Comma-separated list of negative words\n",
    "      - Number of prompts that resulted in a follow-up question\n",
    "      - Number of prompts that resulted in a user expressing satisfaction\n",
    "      - Number of prompts that resulted in a user expressing dissatisfaction\n",
    "      - Number of prompts that resulted in a user asking for help\n",
    "      - Number of prompts that resulted in a user asking for a definition\n",
    "\n",
    "      The output should be in JSON format. Here are two examples of what the output should look like:\n",
    "      '\n",
    "      Example 1:\n",
    "      \n",
    "        \"number_of_prompts_answered\": 15,\n",
    "        \"number_of_positive_words\": 10,\n",
    "        \"list_of_positive_words\": [\"good, excellent, perfect\"],\n",
    "        \"number_of_negative_words\": 5,\n",
    "        \"list_of_negative_words\": [\"bad, terrible, worse\"],\n",
    "        \"number_of_followup_questions\": 3,\n",
    "        \"number_of_expressions_of_satisfaction\": 2,\n",
    "        \"number_of_expressions_of_dissatisfaction\": 1,\n",
    "        \"number_of_help_requests\": 4,\n",
    "        \"number_\n",
    "        of_definition_requests\": 1\n",
    "      \n",
    "\n",
    "      Example 2:\n",
    "      \n",
    "        \"number_of_prompts_answered\": 20,\n",
    "        \"number_of_positive_words\": 12,\n",
    "        \"list_of_positive_words\": [\"nice, great, wonderful\"],\n",
    "        \"number_of_negative_words\": 3,\n",
    "        \"list_of_negative_words\": [\"problematic, difficult, bad\"],\n",
    "        \"number_of_followup_questions\": 4,\n",
    "        \"number_of_expressions_of_satisfaction\": 3,\n",
    "        \"number_of_expressions_of_dissatisfaction\": 2,\n",
    "        \"number_of_help_requests\": 5,\n",
    "        \"number_of_definition_requests\": 2\n",
    "      \n",
    "      \n",
    "\n",
    "    \"\"\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for thread_id in threads_list:\n",
    "    speech_file_path =f\"audio/{thread_id}_speech.mp3\"\n",
    "    print(thread_id)\n",
    "    conversation = reconstruct_conversation(thread_id)\n",
    "    print(conversation)\n",
    "    response = client.audio.speech.create(\n",
    "    model=\"tts-1\",\n",
    "    voice=\"nova\",\n",
    "\n",
    "    input=conversation\n",
    "    )\n",
    "\n",
    "    response.stream_to_file(speech_file_path)\n",
    "    summary = analyze_conversation(conversation, promptSummary)\n",
    "    metrics = analyze_conversation(conversation, promptMetrics)\n",
    "    print(summary)\n",
    "    print(\"========\")\n",
    "    # Limpiar las anotaciones de Markdown que puedan existir\n",
    "    metrics = metrics.strip().replace(\"```json\", \"\").replace(\"```JSON\", \"\").replace(\"```\", \"\")\n",
    "\n",
    "    # Intentar convertir el texto limpio a un objeto JSON\n",
    "    try:\n",
    "        objeto_json = json.loads(metrics)\n",
    "        print(\"Conversión exitosa:\", objeto_json)\n",
    "\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(\"Error al decodificar JSON:\", e)\n",
    "    print(objeto_json)# Update threads_tb columns thread_summary and thread_metrics\n",
    "    update_thread_info(thread_id, summary, objeto_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autogeneracion de cursos \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discovery\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decouple import config\n",
    "from supabase import create_client\n",
    "url_user: str = config(\"SUPABASE_ADMIN_URL\")\n",
    "key_user: str = config(\"SUPABASE_ADMIN_KEY\")\n",
    "supabase = create_client(supabase_url=url_user,supabase_key= key_user)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyDqRltPWDD4-HUxSJ9FzkEuCQ3T1F2lqKg\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_conversation(course_information, prompt):\n",
    "    \"\"\"\n",
    "    Summarize conversation using Gemini and a prompt\n",
    "    \"\"\"\n",
    "    # Initialize the language model\n",
    "    llm = ChatGoogleGenerativeAI(model=\"gemini-pro\")\n",
    "\n",
    "    # Invoke the chain with the provided conversation\n",
    "    chain = prompt | llm\n",
    "    result = chain.invoke({\"course_information\": course_information})\n",
    "\n",
    "    # Instead of printing, save the result to a variable\n",
    "    conversationAnalysis = result.content  # Adjust based on actual attribute/method to access the text\n",
    "\n",
    "    # Return the summary text\n",
    "    return conversationAnalysis\n",
    "\n",
    "\n",
    "promptSummary = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Analyze the following course information and categorize it into the given format.\n",
    "    {course_information}\n",
    "\n",
    "    You MUST organize the information into the following structure:\n",
    "\n",
    "    categories:\n",
    "      - name: \"Category Name\"\n",
    "        threads:\n",
    "          - \"Thread 1\"\n",
    "          - \"Thread 2\"\n",
    "          - \"Thread 3\"\n",
    "        icon_url: \"Icon URL\"\n",
    "        description: \"Description of the category\"\n",
    "\n",
    "    Use the given course details to fill in the following categories: Data Collection, Data Cleaning, Exploratory Data Analysis, Machine Learning, Statistical Analysis, and Big Data Technologies. Include the respective course modules, topics, and descriptions accordingly.\n",
    "\n",
    "    Here are two examples of the desired output format:\n",
    "\n",
    "    Example 1:\n",
    "    categories:\n",
    "      - name: \"Data Collection\"\n",
    "        threads:\n",
    "          - \"Survey Design and Sampling Methods\"\n",
    "          - \"Web Scraping and APIs\"\n",
    "          - \"Data Storage and Management\"\n",
    "        icon_url: \"https://example.com/icons/data-collection.png\"\n",
    "        description: \"Methods and tools for gathering data\"\n",
    "      - name: \"Data Cleaning\"\n",
    "        threads:\n",
    "          - \"Handling Missing Values\"\n",
    "          - \"Data Transformation and Normalization\"\n",
    "          - \"Outlier Detection and Treatment\"\n",
    "        icon_url: \"https://example.com/icons/data-cleaning.png\"\n",
    "        description: \"Preparing raw data for analysis\"\n",
    "\n",
    "    Example 2:\n",
    "    categories:\n",
    "      - name: \"Exploratory Data Analysis\"\n",
    "        threads:\n",
    "          - \"Descriptive Statistics\"\n",
    "          - \"Data Visualization Techniques\"\n",
    "          - \"Identifying Patterns and Trends\"\n",
    "        icon_url: \"https://example.com/icons/exploratory-data-analysis.png\"\n",
    "        description: \"Analyzing data to uncover insights\"\n",
    "      - name: \"Machine Learning\"\n",
    "        threads:\n",
    "          - \"Supervised Learning Algorithms\"\n",
    "          - \"Unsupervised Learning Techniques\"\n",
    "          - \"Model Evaluation and Validation\"\n",
    "        icon_url: \"https://example.com/icons/machine-learning.png\"\n",
    "        description: \"Building predictive models\"\n",
    "\n",
    "    ALWAYS give me the answer in the specified JSON format.\n",
    "    \"\"\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "courses = supabase.table('courses_tb').select(\"*\").execute().data\n",
    "\n",
    "for course in courses:\n",
    "    name = str(course.get(\"name\", \"\"))\n",
    "    general_objective = str(course.get(\"general_objective\", \"\"))\n",
    "    module_objective = str(course.get(\"module_objective\", \"\"))\n",
    "    syllabus = str(course.get(\"syllabus\", \"\"))\n",
    "    \n",
    "    course_info = name + general_objective + module_objective + syllabus\n",
    "    print(course_info)\n",
    "    response= analyze_conversation(prompt=promptSummary,course_information=course_info)\n",
    "    # Remove both ```JSON\\n and ```json\\n, and \\n```\n",
    "    response = response.replace('```JSON\\n', '').replace('```json\\n', '').replace('\\n```', '').strip()\n",
    "\n",
    "    # Load the JSON string into a Python dictionary\n",
    "    try:\n",
    "        data = json.loads(response)\n",
    "        # Print the JSON object to verify\n",
    "        print(json.dumps(data, indent=2))\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"JSON decoding failed: {e}\")\n",
    "        print(f\"Original response: {response}\")\n",
    "    supabase.table(\"courses_tb\").update({\"categories\":data}).eq(\"id\",course[\"id\"]).execute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    response = supabase.table(table_name).insert(followups).execute()\n",
    "supabase_admin.table(\"courses_tb\").select(\"*\").eq(\"id\",courseid).execute().data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response= analyze_conversation(prompt=promptSummary,course_information=course_info)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove both ```JSON\\n and ```json\\n, and \\n```\n",
    "response = response.replace('```JSON\\n', '').replace('```json\\n', '').replace('\\n```', '').strip()\n",
    "\n",
    "# Load the JSON string into a Python dictionary\n",
    "try:\n",
    "    data = json.loads(response)\n",
    "    # Print the JSON object to verify\n",
    "    print(json.dumps(data, indent=2))\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"JSON decoding failed: {e}\")\n",
    "    print(f\"Original response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supabase.table(\"courses_tb\").update({\"categories\":data}).select(\"*\").eq(\"id\",course[\"id\"]).execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
